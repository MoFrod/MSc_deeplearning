{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up"
      ],
      "metadata": {
        "id": "3YpmRqOvtZKb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otScEIo5Ff-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b5f953-2dee-49dd-c4ff-ee968cc02ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=9ac77a5f52860d3d148781f338e3224e224d33f230c76a2c7e601baad4e7f9ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.5/104.5 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "# Environment setup\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip3 install num2words\n",
        "from num2words import num2words\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "!pip3 install contractions\n",
        "import contractions\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount GoogleDrive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWAiE3JWIpC-",
        "outputId": "7120cb08-f5a9-4258-d251-7cd5a9157655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "_l5pS-kaxseH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/Colab Notebooks/CSC8637/SciFi/internet_archive_scifi_v3.txt','r', encoding=\"utf-8\") as f:\n",
        "  scifi = f.read()\n",
        "\n",
        "num_chars = len(scifi)\n",
        "print(\"Number of characters in the document:\", num_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh7H-yi85FcQ",
        "outputId": "23399d5e-b0d4-42f9-e6ba-a680537e94b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters in the document: 149326361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(scifi))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYA6X0bv5fhS",
        "outputId": "1aa84042-98f0-460a-ca63-06b37e55adc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first 5500 characters of the row\n",
        "first_5500_chars = scifi[:5500]\n",
        "print(first_5500_chars) # Remove the first 5200 as copyright materical and not scifi stories."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N8haUru4e0a",
        "outputId": "6581ef02-eb59-44ea-9c18-a0a1dd02526c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MARCH # All Stories New and Complete Publisher Editor IF is published bi-monthly by Quinn Publishing Company, Inc., Kingston, New York. Volume #, No. #. Copyright # by Quinn Publishing Company, Inc. Application for Entry' as Second Class matter at Post Office, Buffalo, New York, pending. Subscription # for # issues in U.S. and Possessions: Canada # for # issues; elsewhere #. Aiiow four weeks for change of address. All stories appearing in this magazine are fiction. Any similarity to actual persons is coincidental. #c a fcopy. Printed ia U.S. A. A chat with the editor  i #  science fiction magazine called IF. The title was selected after much thought because of its brevity and on the theory it is indicative of the field and will be easy to remember. The tentative title that just morning and couldn't remember it until we'd had a cup of coffee, it was summarily discarded. A great deal of thought and effort lias gone into the formation of this magazine. We have had the aid of several very talented and generous people, for which we are most grateful. Much is due them for their warmhearted assistance. And now that the bulk of the formative work is done, we will try to maintain IF as one of the finest books on the market.  t a great public demand for our magazine. In short, why will you buy IF? We cannot, in honesty, say we will publish at all times the best science fiction in the field. That would not be true. But we will have access to the best stories, and we will get our fair share of works from the best writers. We definitely will not talk \"adult\" or \"juvenile\" relative to our content as we feel such terms are misleading. We would rather think at all times in the terms of \"story\". Some of the greatest escapist literature ever written, Treasure Island for instance, could be put into either category or both. And if Edgar Rice Burroughs is juvenile, then so are we, because the late master has given us some memorable thrills. Frankly, we don't think you'll buy IF because you feel we print better yams than any other mag. You will buy it, we hope, because you like its personality. Every magazine, we feel, does have a definite personality of its own. This personality is usually a reflection of the editors, their way of thinking, their appreciation of tKe market, their interpretation of what you will like best in stories and artwork. We have tried to make IF different from any other science fiction magazine on the stands while still building it along the lines of what every science fiction mag must be. Aside from the letter columns and the editorial, which are departments of field-wide use, we have not copied any feature of any other magazine. We will not, for instance, review fanzines, because we feel that is being most ably done by other mags. Nor will we, as a general practice, review books because that appears to us to be overdone. a personality of our own and hope thereby to establish an affinity with a large number of readers who will remember IF when they buy a science fiction mag as one they like and wish to continue reading. At all times we will hew to the story-line and will exhort with our writers to do the same. As an example, when Howard Browne phoned to talk over the plot for his lead novel in this issue, he described what ivas without doubt a staggering premise, a really startling concept. \"But,\" he mourned, S T suppose I'll have to bend it around to give them the good old conventional ending.\" We told Howard, \"Not for IF, chum. Remember the old creed we live by. A writer may cheat on his wife, but he is ever true to the story-line. He may haul his infant son around by one leg. but he carries a good story-idea like a holy relic. If there is only one logical ending for Twelve Times ZerG, that's the ending we want.\" Therefore, we do not feel the majority of readers necessarily want a happy ending regardless of all else. Not when it is incompatable with the aura of realism created by the writer. A check-list of fiction masterpieces certainly bears this out. The furor created by a little piece called Sorry , Wrong Number would certainly not have been forthcoming had the bedridden lady been rescued in the last paragraph. Romeo and Juliet would have beep nothing more than the smooth effort of the world's greatest writer if Romeo had gotten there in time. Yet, in modern fiction, he gets there in time with such amazing regularity one feels he has memorized at least  a dozen time-tables. The result has been unnumbered carloads of mediocre fiction. Also -- though we don't wish to underscore the point too heavily -- what could more surely have smothered the greatness of Wuthering Heights than a happy ending?  that IF will be a magazine given over to tragedy. W e will only insist that our writers create scenes and climaxes that fix the story rather than cater to that old \"debil\" formula. And in so doing we have an entirely selfish motive. This: As the years go by, we want to look back with personal pride upon an everlengthening list of great stories. So the book you now hold in your hands is a new one titled IF. We hope you will like it -- not for just a day -- not for just a month. But for years to come. pwf  Police grilled him mercilessly # while eyes from a hundred worlds looked on  It was a love-triangle murder that made today's headlines but the answer lay hundreds of thousands of light years away! one of the basement rooms. He moved slowly and with a kind of painful dignity, as a man moves on his w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find how many times 'pwf' appear\n",
        "print(scifi.find('pwf')) # Remove"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fqh-r39z0y-",
        "outputId": "39507bd7-023d-42fc-a5d2-23bf129f26a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the last 2000 characters\n",
        "last_2000_chars = scifi[-2000:]\n",
        "print(last_2000_chars) # Remove the last 1944 as not scifi stories."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17lDRz5OuJ-x",
        "outputId": "38bde45c-eb29-4109-b5bc-138aa27efb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tury undersea mission from which no human has ever returned. Now, # feet down, you learn that an (Publ. ed. #) Ed. by T. E. Dikty # top-notch short stories and novelettes by such well known writers as Robert Bloch. Frank Robinson, Mark Clifton, many others. (Publ. ed. #) Just Mail Coupon In.ll.'i.l ' mi coupon your choice of waul in inhlit imi to \"Satellite!\" One bool, will lie considered your first i led ion for which you'll be billed only I plus a few cents jnon I I I you will be offered the cream of I lie new # to # science lid ion books -- for only I each. You take only those books you rcnlly want -- as few as four a year llut this offer may be with. I I a w I I at any time. So mail Del OX-#, Garden City, N. Y. Dept. GX-#, Garden City, N. Y. Enroll me as a member, and rush me my full-length handcated below. Bill me only I (plus few cents shipping charges). Every month send me the Club's free bulletin, \"Things to Come,\" so that I may decide whether or not I wish to receive the coining monthly selection described therein. For each book take a book every month (only four during each year I am a member) -- and I may resign at any time after accepting four selections. return all books in # days, pay nothing, and this membership will be cancelled.  Astounding S-F Anthology  Dragon in the Sea  Best from Fantasy and S-F  Omnibus of S-F  Best S-F Stories and Novels  Treasury of S-F Classics Name AddressCity (please print) . Zone StateSelection price in Canada # plus shipping. Address Science-Fiction Book Club (Canada i, # Bond St. Toronto #. (Good only in Continental LI. S. and Canada) e comfactors v This h  s every # f astounding J a  fiL And the  ealth S sV-i I ?. answersScience-Fiction Book T .f  ?s # on Jhe fM # than-fiction  , ob , e ik storv of our , available Th e fascinating . St ; y acts not even available... authoritative iournals n etey TO # \" I  s i two years' But # t w# rk? about gout predict mysterious What .V. first hook  -- See oier side for full details \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to calculate average sentence length\n",
        "def average_sentence_len(text):\n",
        "  sentences = text.split(\".\")\n",
        "  words = text.split(\" \")\n",
        "  if (sentences[len(sentences)-1]==\"\"):\n",
        "    average_sentence_length= len(words) / len(sentences)-1\n",
        "  else:\n",
        "    average_sentence_length = len(words) / len(sentences)\n",
        "\n",
        "  return average_sentence_length\n",
        "\n",
        "# Define function to calculate maximum sentence length\n",
        "def max_sentence_len(text):\n",
        "  sentences = text.split(\".\")\n",
        "  max_len = max([len(sentence.split()) for sentence in sentences])\n",
        "  return max_len"
      ],
      "metadata": {
        "id": "8Ro2RM3q-7j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max and average sentence length\n",
        "average_sentence_length = average_sentence_len(scifi)\n",
        "max_sentence_length = max_sentence_len(scifi)\n",
        "print(f\"Average sentence length: {average_sentence_length}\")\n",
        "print(f\"Maximum sentence length: {max_sentence_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lRjiG6va79I",
        "outputId": "34d0896c-aa9a-43b0-ebd2-e98433085d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sentence length: 12.553956848085688\n",
            "Maximum sentence length: 326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many words are in the text\n",
        "word_count = len(scifi.split(\" \"))\n",
        "print(\"The number of words in the sample text is:\", word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di1lV6KB8gBv",
        "outputId": "94228179-5285-4b6f-edff-431905887a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words in the sample text is: 26655101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "w9SFsQF2xznA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to convert numbers to text\n",
        "def convert_numbers_to_text(text):\n",
        "    numbers = re.findall(r'\\d+', text)\n",
        "    for number in numbers:\n",
        "        text = text.replace(number, num2words(int(number)))\n",
        "    return text"
      ],
      "metadata": {
        "id": "8OJ7ggz1xxY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to clean and preprocess the text\n",
        "def preprocess(text):\n",
        "  text = text.replace(text[:5200], '') # Delete start of dataset\n",
        "  text = text.replace(text[-1994:], '') # Delete end of dataset\n",
        "  text = text.replace('pwf', '')\n",
        "  text = convert_numbers_to_text(text)\n",
        "  text = text.replace('&', 'and')\n",
        "  text = text.replace('%', ' percent')\n",
        "  text = contractions.fix(text)\n",
        "  # A list of special charaters to be removed\n",
        "  special_characters = ['@', '#', '$', '£', '*', '-', '+', '=', '~', '<', '>']\n",
        "  for i in special_characters:\n",
        "    if i != ',' or '.':\n",
        "      text = text.replace(i, \"\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "izU4LGgz2nKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the original text\n",
        "clean_scifi = preprocess(scifi)"
      ],
      "metadata": {
        "id": "WsqZQpuX5OAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the preprocessed text to a file\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/CSC8637/SciFi/clean_scifi.txt', 'w', encoding='utf-8') as f:\n",
        "  f.write(clean_scifi)"
      ],
      "metadata": {
        "id": "eWP45JkuJ53n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load preprocessed data\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/CSC8637/SciFi/clean_scifi.txt','r', encoding=\"utf-8\") as f:\n",
        "  clean_scifi = f.read()"
      ],
      "metadata": {
        "id": "qOoYntVSNbeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first 300 characters to check it's correct\n",
        "first_300_chars = clean_scifi[:300]\n",
        "print(first_300_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blI6M9H2HmaI",
        "outputId": "a74611bf-27a4-4eb2-e51f-68285aeb61ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Police grilled him mercilessly  while eyes from a hundred worlds looked on  It was a lovetriangle murder that made today's headlines but the answer lay hundreds of thousands of light years away! one of the basement rooms. He moved slowly and with a kind of painful dignity, as a man moves on his way\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max and average sentence length\n",
        "average_sentence_length = average_sentence_len(clean_scifi)\n",
        "max_sentence_length = max_sentence_len(clean_scifi)\n",
        "print(f\"Average sentence length: {average_sentence_length}\")\n",
        "print(f\"Maximum sentence length: {max_sentence_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWCqOyPaZ1FD",
        "outputId": "3a8ab5cc-41da-4e06-aa07-2c185a543012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sentence length: 12.755057726536492\n",
            "Maximum sentence length: 321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "clean_vocab = sorted(set(clean_scifi))\n",
        "print(f'{len(clean_vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ215IHDY3zn",
        "outputId": "49f75367-adc4-4449-b801-847c67a40f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many words are in the text\n",
        "clean_word_count = len(clean_scifi.split(\" \"))\n",
        "print(\"The number of words in the text is:\", clean_word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjXQQbzd8zPS",
        "outputId": "f9ddf2a3-59ff-46bf-8f70-82d027b607c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words in the text is: 27073809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique words\n",
        "\n",
        "def countUniqueWords(preprocessed_text):\n",
        "    words_in_file = preprocessed_text.split(\" \")\n",
        "    count_map = {}\n",
        "    for i in words_in_file:\n",
        "        if i in count_map:\n",
        "            count_map[i] += 1\n",
        "        else:\n",
        "            count_map[i] = 1\n",
        "    count = 0\n",
        "    for i in count_map:\n",
        "        if count_map[i] == 1:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# Assume that clean_scifi contains the preprocessed text\n",
        "unique_words_count = countUniqueWords(clean_scifi)\n",
        "print('Number of unique words in the file are:', unique_words_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVNvK_FMu3tl",
        "outputId": "894f0bfc-6e0d-4d35-ef32-fa1936f865b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in the file are: 385901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "!pip install pyyaml h5py\n",
        "import h5py\n",
        "\n",
        "def save_tokenizer(tokenizer, filename):\n",
        "    with h5py.File(filename, 'w') as f:\n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            f[key] = np.array([value])\n",
        "\n",
        "def load_tokenizer(filename):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
        "    with h5py.File(filename, 'r') as f:\n",
        "        tokenizer.word_index = {key: int(value[()]) for key, value in f.items()}\n",
        "    return tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlHGu_vkyh1O",
        "outputId": "fa87fd9e-8888-4d4c-f3c3-701cb1790c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from h5py) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(clean_scifi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrsTsWxVEiNp",
        "outputId": "e46bad5e-2a92-45a1-aa0a-0b37cbaac934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_scifi.split(' ')[:20000])\n",
        "\n",
        "#save_tokenizer(tokenizer, '/content/gdrive/MyDrive/Colab Notebooks/CSC8637/checkpoints/tokenizer.h5')"
      ],
      "metadata": {
        "id": "XWu6ZEfx2h8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "#tokenizer = load_tokenizer('/content/gdrive/MyDrive/Colab Notebooks/CSC8637/checkpoints/tokenizer.h5')\n"
      ],
      "metadata": {
        "id": "FUqVvGw-zH48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKF-vtzkTsF0",
        "outputId": "4dc621f7-6d75-4bc3-9533-30373d82ee5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define a function to convert the text into sequences and pad them\n",
        "def tokenize_and_pad(text):\n",
        "    # Convert the text into sequences\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    # Pad the sequences\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=21, padding='pre')\n",
        "    return padded_sequences\n",
        "\n",
        "# def generate_batches(text, token_batch_size):\n",
        "#   seq, labels = [], []\n",
        "#   for i in range(0, len(text), token_batch_size):\n",
        "#     for j in range(i, i+token_batch_size):\n",
        "#       chunk = text[i:j+2]\n",
        "#       sequences = tokenize_and_pad(chunk)\n",
        "#       #yield sequences[:,:-1], keras.utils.to_categorical(sequences[:,-1], num_classes = vocab_size)\n",
        "#       seq.append([sequences[:-1]])\n",
        "#       labels.append(keras.utils.to_categorical(sequences[-1], num_classes = vocab_size))\n",
        "#   return seq, labels\n",
        "\n",
        "def generate_batches(text, token_batch_size):\n",
        "  sequences = np.array(tokenize_and_pad(text))\n",
        "  seq, labels = sequences[:,:-1], sequences[:,-1]\n",
        "  labels = keras.utils.to_categorical(labels, num_classes=vocab_size)\n",
        "  return seq, labels"
      ],
      "metadata": {
        "id": "attDQGc74kyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_batch_size = 20\n",
        "\n",
        "# Tokenize and pad the train, validation, and test text\n",
        "generated_text = generate_batches(clean_scifi[:8000].split(' '), token_batch_size)\n",
        "\n",
        "sequences, labels = generated_text"
      ],
      "metadata": {
        "id": "z2ITRwoz3XFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "EVqt0yAjtRU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=20))\n",
        "model.add(LSTM(640, return_sequences=True))\n",
        "model.add(Dense(1280, activation=\"relu\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "M-UbuwFn3rVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MI8gnx1481C",
        "outputId": "782e20df-24d5-41b8-e804-923b45fcfb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 20, 10)            35280     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 20, 640)           1666560   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20, 1280)          820480    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25600)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3528)              90320328  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 92,842,648\n",
            "Trainable params: 92,842,648\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), run_eagerly=True)\n",
        "\n",
        "model.fit(sequences, labels, epochs=20, batch_size=64, steps_per_epoch=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1HLZxp24J7O",
        "outputId": "15b299df-3dee-403b-db04-de01e27fa8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fabbf285280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fabbf285280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 15s 78ms/step - loss: 6.1416\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 5.7524\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 5s 53ms/step - loss: 5.7298\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 6s 61ms/step - loss: 5.7196\n",
            "Epoch 5/20\n",
            " 80/100 [=======================>......] - ETA: 1s - loss: 5.7202"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 43ms/step - loss: 5.7202\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabbf36faf0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format\n",
        "model.save('/content/gdrive/MyDrive/Colab Notebooks/CSC8637/models/lstm.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TixDiJyO_62X",
        "outputId": "fe77ca56-c36e-4e1a-8b12-a74aa32cefd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from h5py) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Text"
      ],
      "metadata": {
        "id": "7w1oF_KetTzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = tf.keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/CSC8637/models/lstm.h5')"
      ],
      "metadata": {
        "id": "jdbUOt_0ANuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate text\n",
        "\n",
        "def generate_text(model, tokenizer, seed_sequence, num_words):\n",
        "    \"\"\"\n",
        "    Generate text using the specified model, tokenizer, seed sequence, and number of words to generate.\n",
        "\n",
        "    Args:\n",
        "    - model: The trained Keras model to use for text generation.\n",
        "    - tokenizer: The Keras Tokenizer used to convert text to sequences.\n",
        "    - seed_sequence: The seed sequence to use for text generation, as a string.\n",
        "    - num_words: The number of words to generate after the seed sequence.\n",
        "\n",
        "    Returns:\n",
        "    - The generated text as a string.\n",
        "    \"\"\"\n",
        "    # Convert the seed sequence to a sequence of integer tokens\n",
        "    #seed_tokens = tokenizer.texts_to_sequences([seed_sequence])[0]\n",
        "\n",
        "    # Generate num_words additional tokens using the model\n",
        "    probs_arr = []\n",
        "    for i in range(num_words):\n",
        "        seed_tokens = tokenizer.texts_to_sequences([seed_sequence])\n",
        "        padded_seed = pad_sequences(seed_tokens, maxlen=20, padding='pre')\n",
        "\n",
        "        # Predict the next token(s) given the current sequence\n",
        "        predicted_probs = model.predict(padded_seed)\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "        #print(f'word:{word}, index:{index}')\n",
        "          if index == np.argmax(predicted_probs, axis=-1):\n",
        "            output_word = word\n",
        "            break\n",
        "        seed_sequence += \" \"+output_word\n",
        "\n",
        "    return seed_sequence\n"
      ],
      "metadata": {
        "id": "eM4I4yK-EARl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example prompts:\n",
        "#My only thought was to hide\n",
        "#She shook her head emphatically\n",
        "#handle it wisely\n",
        "#At first, of course, you will rebel.\n",
        "#I was smart enough to fool all the so-called brains of the Solar System\n",
        "\n",
        "\n",
        "# Generate text using the model and tokenizer\n",
        "seed_sequence = \"I was smart enough to fool all the so-called brains of the Solar System.\"\n",
        "num_words = 20\n",
        "generated_text = generate_text(model, tokenizer, seed_sequence=seed_sequence, num_words=num_words)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGXrrcOAE3q4",
        "outputId": "6b866a39-8939-40e8-96ab-fac0c74c2ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "I was smart enough to fool all the so-called brains of the Solar System. the the the the the the the the the the the the the the the the the the the the\n"
          ]
        }
      ]
    }
  ]
}